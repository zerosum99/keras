{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark\n"
     ]
    }
   ],
   "source": [
    "# %load requirements.txt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "print(findspark.find())\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"Spark Practice 1 \")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 참조자료 \n",
    "\n",
    "https://wikidocs.net/28532"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 직접 데이터프레임 생성하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(spark, \"createDataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, StringType\n",
    "\n",
    "df = spark.createDataFrame([1.0, 2.0, 3.0], StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.session.SparkSession.createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SparkSession.__dict__['createDataFrame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SparkSession.createDataFrame of <pyspark.sql.session.SparkSession object at 0x11a0ce1d0>>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.250.64.91:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Practice 1 </code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11a0ce1d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.250.64.91:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Practice 1 </code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x11a0ce1d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame.__self__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|  1.0|\n",
      "|  2.0|\n",
      "|  3.0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  RDD에서 데이터프레임으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1,2,3),(4,5,6),(7,8,9)])\n",
    "df = rdd.toDF([\"a\",\"b\",\"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(rdd, \"toDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_is_barrier',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_memory_limit',\n",
       " '_pickled',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'barrier',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'isLocallyCheckpointed',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'localCheckpoint',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDF',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.__getattribute__('show')()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: long (nullable = true)\n",
      " |-- c: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row 클래스를 사용해서 행 추가하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "rdd = sc.parallelize([Row(a=1,b=2,c=3),Row(a=4,b=5,c=6),Row(a=7,b=8,c=9)])\n",
    "df_ = rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  a|  b|  c|\n",
      "+---+---+---+\n",
      "|  1|  2|  3|\n",
      "|  4|  5|  6|\n",
      "|  7|  8|  9|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleRDD = sc.parallelize( [ (\"David\", 150), (\"White\", 200), (\"Paul\",  170)  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[32] at parallelize at PythonRDD.scala:195"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleRDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  두개의 칼럼 정보를 리스트에 넣고 처리할 것 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = peopleRDD.toDF( [\"name\", \"salary\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, salary: bigint]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|David|   150|\n",
      "|White|   200|\n",
      "| Paul|   170|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RDD에  스키마 만들고 데이터프레임으로 변환하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleRDD = sc.parallelize([Row(\"David\", 150),Row(\"White\", 200),Row(\"Paul\",  170) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleSchema = StructType().add(StructField(\"name\",   StringType(), True)).add(StructField(\"salary\", IntegerType(), True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = spark.createDataFrame(peopleRDD, peopleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "| name|salary|\n",
      "+-----+------+\n",
      "|David|   150|\n",
      "|White|   200|\n",
      "| Paul|   170|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 프레임의 스키마 정보 확인\n",
    "\n",
    "df.printSchema() 로 스키마를 확인해볼 수 있다.\n",
    "\n",
    "df.columns 로 컬럼을 확인해 볼 수 있고, 파이썬 리스트로 반환된다.\n",
    "\n",
    "df.describe() 로 데이터프레임 내용을 확인해 볼 수 있다. 이 메서드로는 데이터프레임이 리턴되는것을 확인해볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.printSchema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'salary']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, name: string, salary: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON 파일을 만들어서 데이터프레임으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting people.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile people.json\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"./people.json\")\n",
    "\n",
    "# df = dataframe!\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 텍스트 파일을 만들어서 데이터프레임으로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting people.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile people.txt\n",
    "A, 29\n",
    "B, 30\n",
    "C, 19\n",
    "D, 15\n",
    "F, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleRDD = sc.textFile(\"./people.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "./people.txt MapPartitionsRDD[64] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleRDD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A, 29', 'B, 30', 'C, 19', 'D, 15', 'F, 20']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleRDD .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스키마 정보 입력하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleSchema = StructType().add(StructField(\"name\",   StringType(), True)).add(StructField(\"age\", IntegerType(), True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 파일을 분리해서 Row 클래스로 변호나하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sepPeopleRdd = peopleRDD.map(lambda line  :  line.split(\",\")).map(lambda x  :  Row(x[0], int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF = spark.createDataFrame(sepPeopleRdd, peopleSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   A| 29|\n",
      "|   B| 30|\n",
      "|   C| 19|\n",
      "|   D| 15|\n",
      "|   F| 20|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### df.select('age').show() 로 위 컬럼의  데이터를 가져올 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.select(\"name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [age#210]\n",
      "+- *(1) Filter (isnotnull(age#210) && (age#210 > 20))\n",
      "   +- Scan ExistingRDD[name#209,age#210]\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select(\"age\").where(\"age > 20\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [age#210]\n",
      "+- Scan ExistingRDD[name#209,age#210]\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select(\"age\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 29|\n",
      "| 30|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select(\"age\").where(\"age > 20\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   스칼라 언어 처리 : \n",
    "\n",
    "      df.select($\"name\", $\"age\" + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [name#209, (age#210 + 1) AS (age + 1)#304]\n",
      "+- Scan ExistingRDD[name#209,age#210]\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select(peopleDF.name, peopleDF.age + 1).explain() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "|name|(age + 1)|\n",
      "+----+---------+\n",
      "|   A|       30|\n",
      "|   B|       31|\n",
      "|   C|       20|\n",
      "|   D|       16|\n",
      "|   F|       21|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select(peopleDF.name, peopleDF.age + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### df.select(['age', 'name']) 의 식으로 다중 컬럼을 선택할 수도 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   A| 29|\n",
      "|   B| 30|\n",
      "|   C| 19|\n",
      "|   D| 15|\n",
      "|   F| 20|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select([\"name\",\"age\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### df.head(2) 로 위에 있는 두 가지 row를 가져올 수 있다. df.head(2)[0] 의 식으로 파이썬에서 하던 인덱싱을 할 수도 있다. 아주 간편하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='A', age=29)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.head()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF.head()['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필터 처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   A| 29|\n",
      "|   B| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.filter(\"age> 21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|   A| 29|\n",
      "|   B| 30|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.select([\"name\", \"age\"]).filter(\"age > 21\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupby 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 20|    1|\n",
      "| 19|    1|\n",
      "| 15|    1|\n",
      "| 29|    1|\n",
      "| 30|    1|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "peopleDF.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메소드들 \n",
    "\n",
    "컬럼을 좀 더 다뤄보자.\n",
    "\n",
    "df.withColumn(\"새컬럼\", df['age']) 으로 새 컬럼을 만들 수 있다. withColumn() 은 아규먼트로 컬럼 이름과, sql.컬럼을 받기 때문에 주의한다.\n",
    "\n",
    "df.withColumn(\"두배', df['age']*2) 와 같이 컬럼 내 데이터들을 손봐서 새 컬럼을 추가하는 데 사용할 수 있다.\n",
    "\n",
    "df.withColumnRenamed(\"age\", \"renamed_age\").show() 로 컬럼 이름을 바꾼 후 내용을 확인해볼 수 있다.\n",
    "\n",
    "sql 모듈에서는 sql을 다루는 쉬운 메서드들이 많다.\n",
    "\n",
    "df.createOrReplaceTempView(\"새로만들 테이블\") 로 테이블을 만들듯 sql 임시 뷰를 만들 수 있다.\n",
    "\n",
    "results = spark.sql(\"select * from 새로만들 테이블\") 식으로, sql을 다루듯 사용할 수 있으므로 별다른 많은 공부를 요구하지 않는다. 아주 쉽다. 아래와 같이 제플린에서 사용한다면 될 것이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### df['age'] 로 인덱싱을 해 보면 파이스파크 컬럼을 얻을 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'name'>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peopleDF['name']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
